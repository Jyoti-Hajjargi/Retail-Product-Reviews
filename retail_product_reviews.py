# -*- coding: utf-8 -*-
"""Retail Product Reviews.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ar_V5wnEBKo-XWv3kTp06d70fz6G3po2

# Importing required libraries and setting viewing options
"""

import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
import math
import scipy.stats as st
from sklearn.preprocessing import MinMaxScaler

import warnings
warnings.filterwarnings("ignore")

pd.set_option('display.max_columns', None)
pd.set_option('display.width', 500)
pd.set_option('display.max_rows', 30)
pd.set_option('display.float_format', lambda x: '%.5f' % x)

from google.colab import files
uploaded = files.upload()

"""Reading the csv file and viewing the dataset to have a general idea about it"""

df_ = pd.read_csv("Reviews.csv")
df = df_.copy()

def view_data(dataframe):
    print("########################## HEAD ##########################")
    print(dataframe.head())
    print("########################## INFO ##########################")
    print(dataframe.info())
    print("########################## SHAPE ##########################")
    print(dataframe.shape)
    print("########################## ISNULL(?) ##########################")
    print(dataframe.isnull().sum())
    print("########################## DESCRIBE ##########################")
    print(dataframe.describe().T)
    print("####################################################")

view_data(df)

"""Preparing the data to do accurate analysis of it"""

def data_prep(dataframe):
    #Renaming "Score" variable to "Rating"
    dataframe =  dataframe.rename(columns = {"Score":"Rating"})
    dataframe.head()

    #Converting Time to Datetime
    dataframe["Time"] = pd.to_datetime(dataframe["Time"], unit="s")

    #Checking latest date in the dataset to set my current analysis date to +2 days from it.
    dataframe["Time"].max()
    dataframe["Time"].min()

    current_date = pd.to_datetime("2012-10-28")
    dataframe["days_since"] = (current_date - dataframe["Time"]).dt.days

    return dataframe

df = data_prep(df)

"""Grouping the products and calculating their average rating without any weighted calculation"""

df_products =  df.groupby("ProductId").agg({"Rating":"mean",
                             "HelpfulnessDenominator": "count"})
df_products = df_products.rename(columns={"Rating": "Average_Rating", "HelpfulnessDenominator": "Comment_Count"})
print(df_products.head())

"""Sorting based on their Rating_Average"""

print(df_products.sort_values("Average_Rating", ascending=False).head(10))

"""Sorting based on their Comment_Count"""

print(df_products.sort_values("Comment_Count", ascending = False).head(10))

"""Scaling comment counts between 1-5"""

df_products["Comment_Count_scaled"] = MinMaxScaler(feature_range=(1 ,5)).fit(df_products[["Comment_Count"]]).transform(df_products[["Comment_Count"]])
print(df_products.head())

"""Applying log transformation to 'Comment_Count' to reduce the effect of outliers"""

df_products['Comment_Count_log'] = np.log1p(df_products['Comment_Count'])  # log1p adds 1 to avoid log(0)

"""Fitting these log values between 1-5"""

df_products['Comment_Count_scaled'] = MinMaxScaler(feature_range=(1, 5)).fit(df_products[["Comment_Count_log"]]).transform(df_products[['Comment_Count_log']])
df_products.drop("Comment_Count_log", axis = 1, inplace = True)
print(df_products.head())

"""Calculating the weighted sorting score based on ratings and comments"""

def weighted_sorting_score(dataframe, rating_w = 50, comment_w = 50):
    return (dataframe["Average_Rating"] * rating_w / 100 + dataframe["Comment_Count_scaled"] * comment_w / 100)

df_products["weighted_sorting_score"] = weighted_sorting_score(df_products)

"""Top 10 Products Listed and Sorted According to the Weighted Sorting Score"""

df_products.sort_values("weighted_sorting_score", ascending = False).head(10)

"""Assuming that we are only focused on one specific product and sorting its reviews"""

df_filtered = df[df["ProductId"] == "B0026RQTGE"]

"""Setting analysis date according to the max() date"""

df_filtered["Time"].max()
#To be doing the analysis 2 days after the most recent data
df_filtered["days_since"] = df_filtered["days_since"] - 1

"""Calculating up and down values from the variables"""

df_filtered["up"] = df_filtered["HelpfulnessNumerator"]
df_filtered["down"] = df_filtered["HelpfulnessDenominator"] - df_filtered["HelpfulnessNumerator"]

"""Calculating the Wilson Lower Bound Score to sort the reviews from most helpful to least"""

def wilson_lower_bound(up, down, confidence = 0.95):
    n = up + down #if no votes at all
    if n == 0:
        return 0
    z = st.norm.ppf(1 - (1 - confidence) / 2)
    phat = 1.0 * up / n

    return (phat + z * z / (2 * n) - z * math.sqrt((phat * (1 - phat) + z * z / (4 * n)) / n)) / (1 + z * z / n)

df_filtered["wilson_lower_bound"] = df_filtered.apply(lambda x: wilson_lower_bound(x["up"], x["down"]), axis = 1)

"""Sorting the product's reviews based on calculated WLB scores"""

df_filtered.sort_values(by=["wilson_lower_bound", "days_since"], ascending = [False, True]).head(10)

"""To account for recency, applying a weighted review sorting method"""

scaled_days_since = MinMaxScaler(feature_range=(0, 1)).fit(df_filtered[["days_since"]]).transform(df_filtered[["days_since"]])

# Reversing the scaling to make highest 'days_since' = 1 and lowest 'days_since' = 5
df_filtered["days_since_scaled"] = 1 - scaled_days_since

"""Defining a weighted sorting function based on time and WLB scores"""

def weighted_review_sorting(dataframe, WLB_w = 70, time_w = 30):
    return (dataframe["days_since_scaled"] * time_w / 100 + dataframe["wilson_lower_bound"] * WLB_w / 100)

df_filtered["weighted_review_sorting"] = weighted_review_sorting(df_filtered, 65, 35)

"""Sorting the reviews once more with the weighted review sorting to check the results"""

df_filtered.sort_values(by="weighted_review_sorting", ascending=False).head(10)